{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet d'import des données Historique Zendesk vers BigQuery\n",
    "\n",
    "## Introduction\n",
    "1. Imports des bibliothèques nécessaires\n",
    "2. Définition des variables et des configurations\n",
    "3. Manipulation des données JSON\n",
    "4. Création des fonctions pour le chargement des données\n",
    "5. Exécution du processus de chargement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Début du chargement des bibliothèques ...\n",
      "... Bibliothèques chargées ...\n"
     ]
    }
   ],
   "source": [
    "print(\"... Début du chargement des bibliothèques ...\")\n",
    "import json                         # Manipulation JSON -> Déjà prévu dans Python\n",
    "import pandas as pd                 # Manipulation données -> pip install pandas\n",
    "\n",
    "#instalation Gcoud CLI + créer des id locaux pour mon compte Google\n",
    "# Documentations :\n",
    "# - https://cloud.google.com/bigquery/docs/authentication?hl=fr\n",
    "# - https://cloud.google.com/sdk/docs/install?hl=fr\n",
    "# - https://cloud.google.com/bigquery/docs/authentication/getting-started?hl=fr\n",
    "\n",
    "from google.cloud import bigquery   # Liaison BigQuery -> pip install google-cloud-bigquery\n",
    "print(\"... Bibliothèques chargées ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variables et configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fichiers JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportZendesk1 = 'file1.json'   # Export\n",
    "exportZendesk2 = 'file2.json'   # Données en base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#project_id = \"your_projectID\"                          # Identifiant du projet BigQuery\n",
    "#dataset_id = \"your_datasetID\"                  # Identifiant du dataset BigQuery\n",
    "#table_name = \"your_tablename\"                                   # Nom de la table de destination\n",
    "#json_file_path = \"your_path\"                     # Chemin d'accès à l'export JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manipulation données JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge des 2 fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... Ouverture du premier fichier en cours ...\")\n",
    "with open(exportZendesk1) as f:  \n",
    "    data1 = json.load(f)\n",
    "print(\"... Premier fichier ouvert ...\")  \n",
    "\n",
    "print(\"... Ouverture du deuxième fichier en cours ...\")  \n",
    "with open(exportZendesk2) as f:  \n",
    "    data2 = json.load(f)\n",
    "print(\"... Deuxième fichier ouvert ...\")  \n",
    "\n",
    "print(\"... Fusion des 2 fichiers ...\")\n",
    "data1.update(data2)\n",
    "print(\"... Fusion terminé ...\")\n",
    "\n",
    "print(\"... écriture du nouveau fichier ...\")\n",
    "with open('merged.json', 'w') as f:  \n",
    "    json.dump(data1, f)\n",
    "print(\"... écriture du nouveau fichier terminé ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vérification et suppression des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonctions (chargement et insertion des données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Initialisation du client BigQuery ...\n",
      "... Initialisation terminé\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... Initialisation terminé\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Référence au dataset et à la table\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m dataset_ref \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mdataset(dataset_id)\n\u001b[0;32m     10\u001b[0m table_ref \u001b[38;5;241m=\u001b[39m dataset_ref\u001b[38;5;241m.\u001b[39mtable(table_name)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Configuration du job de chargement\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialisation du client BigQuery avec une clé de compte de service\n",
    "print(\"... Initialisation du client BigQuery ...\")\n",
    "#credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "#client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "client = bigquery.Client()\n",
    "print(\"... Initialisation terminé\")\n",
    "\n",
    "# Référence au dataset et à la table\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(table_name)\n",
    "\n",
    "# Configuration du job de chargement\n",
    "print(\"... Confirguration ...\")\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "print(\"... Configuration terminé ...\")\n",
    "\n",
    "# Chargement des données JSON dans la table BigQuery\n",
    "print(\"... Début du chargement des données ...\")\n",
    "with open(json_file_path, \"rb\") as source_file:                                             # rb = read binary => meilleur compatibilité et intégrité des données\n",
    "    load_job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "load_job.result()                                                                           # Attendre la fin du job de chargement\n",
    "print(\"... Chargement des données terminé ...\")\n",
    "\n",
    "print(f\"Les données du fichier {json_file_path} ont été chargées avec succès dans {table_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
